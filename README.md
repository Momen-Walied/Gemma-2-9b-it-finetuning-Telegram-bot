# ğŸ‡ªğŸ‡¬ Gemma-2-9B Egyptian Arabic Translator Bot

<div align="center">

![Gemma Logo](https://img.shields.io/badge/Gemma-2--9B--IT-blue?style=for-the-badge&logo=google&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white)
![Telegram](https://img.shields.io/badge/Telegram-Bot-blue?style=for-the-badge&logo=telegram&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-red?style=for-the-badge&logo=pytorch&logoColor=white)
![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow?style=for-the-badge)

*Fine-tuned Google Gemma-2-9B model for English to Egyptian Arabic dialect translation with Telegram bot integration*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-project-structure) â€¢ [ğŸ¥ Video Demo](#-video-demo) â€¢ [ğŸ¤– Bot Demo](#-telegram-bot-demo) â€¢ [ğŸ”§ Installation](#-installation)

</div>

---

## ğŸŒŸ Overview

This project demonstrates the complete pipeline of fine-tuning Google's **Gemma-2-9B-IT** model to translate English text into authentic **Egyptian Arabic dialect** (Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©). The project includes data preparation, model fine-tuning using LoRA (Low-Rank Adaptation), and deployment as an interactive Telegram bot.

### âœ¨ Key Features

- ğŸ¯ **Specialized Translation**: Fine-tuned specifically for Egyptian Arabic dialect
- ğŸš€ **Efficient Training**: Uses LoRA and 4-bit quantization for memory-efficient training
- ğŸ¤– **Telegram Integration**: Ready-to-deploy bot for real-time translation
- ğŸ“Š **Complete Pipeline**: From data preparation to deployment
- ğŸ’¾ **Chat History**: Saves user conversations for analysis
- âš¡ **Fast Inference**: Optimized for real-time responses

## ğŸ¯ What Makes This Special?

Unlike generic Arabic translators, this model is specifically trained on **Egyptian dialect** (Ø§Ù„Ù…ØµØ±ÙŠ), capturing:
- Colloquial expressions and slang
- Cultural context and idioms
- Natural conversational flow
- Regional linguistic nuances

## ğŸ¥ Video Tutorial

<div align="center">

[![YouTube Video Tutorial](https://img.shields.io/badge/ğŸ¥-Watch%20Demo%20on%20YouTube-red?style=for-the-badge&logo=youtube&logoColor=white)](https://youtu.be/3hjUMs2JJak)

**Watch the complete walkthrough of the project from training to deployment!**

</div>

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- Telegram Bot Token

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/Gemma-2-9b-it-finetuning-Telegram-bot.git
cd Gemma-2-9b-it-finetuning-Telegram-bot

# Install dependencies
pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git
pip install xformers trl peft accelerate bitsandbytes triton
pip install pyTelegramBotAPI transformers datasets pandas
```

### Quick Demo

```python
# Load the fine-tuned model
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="English2Egyptian",  # Your fine-tuned model
    max_seq_length=2048,
    dtype=None,
)

# Translate English to Egyptian Arabic
query = 'Translate to Egyptian dialect: "I love Egypt and its people"'
# Output: "Ø£Ù†Ø§ Ø¨Ø­Ø¨ Ù…ØµØ± ÙˆØ´Ø¹Ø¨Ù‡Ø§" ğŸ‡ªğŸ‡¬
```

## ğŸ“– Project Structure

The project is organized into 4 comprehensive parts:

### ğŸ“ Part 1: Model Loading
```
part 1 - load models/
â”œâ”€â”€ part-1.ipynb          # Load and test base Gemma-2-9B model
```
- Loads Google's Gemma-2-9B-IT model
- Sets up 4-bit quantization for memory efficiency
- Tests base model capabilities

### ğŸ“ Part 2: Dataset Creation
```
part 2 - create dataset/
â”œâ”€â”€ part-2.ipynb          # Data preprocessing and formatting
â”œâ”€â”€ data.csv              # English-Egyptian Arabic parallel corpus
```
- Processes parallel English-Egyptian Arabic dataset
- Creates properly formatted training prompts
- Splits data into train/test sets (80/20)

### ğŸ“ Part 3: Fine-tuning Process
```
part 3 - fine tuning process/
â”œâ”€â”€ part-3.ipynb          # LoRA fine-tuning implementation
```
- Implements LoRA (Low-Rank Adaptation) for efficient training
- Configures training parameters and optimization
- Saves the fine-tuned model as "English2Egyptian"

### ğŸ“ Part 4: Telegram Bot
```
part 4 - Telegram/
â”œâ”€â”€ Part4.ipynb           # Telegram bot implementation
```
- Creates interactive Telegram bot
- Implements chat history saving
- Handles real-time translation requests

## ğŸ”§ Technical Details

### Model Architecture
- **Base Model**: Google Gemma-2-9B-IT
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Quantization**: 4-bit for memory efficiency
- **Context Length**: 2048 tokens

### Training Configuration
```python
# LoRA Configuration
r = 16                    # Rank
lora_alpha = 16          # Alpha parameter
lora_dropout = 0         # Dropout (optimized)
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]

# Training Parameters
batch_size = 2
gradient_accumulation_steps = 4
learning_rate = 2e-5
epochs = 10
```

### Dataset Format
The training data follows this structure:
```json
{
  "instruction": "ØªØ±Ø¬Ù… Ø§Ù„ÙŠ Ø§Ù„Ù„Ù‡Ø¬Ù‡ Ø§Ù„Ù…ØµØ±ÙŠÙ‡",
  "input": "I love Egypt and its people",
  "output": "Ø£Ù†Ø§ Ø¨Ø­Ø¨ Ù…ØµØ± ÙˆØ´Ø¹Ø¨Ù‡Ø§"
}
```

## ğŸ¤– Telegram Bot Demo

### Bot Features
- **Welcome Message**: Greets users in Arabic
- **Real-time Translation**: Instant English to Egyptian Arabic translation
- **Chat History**: Saves all conversations in JSON format
- **Error Handling**: Robust error management and user feedback

### Sample Conversation
```
User: "Hello, how are you today?"
Bot: "Ø£Ù‡Ù„Ø§Ù‹ØŒ Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ"

User: "I want to visit the pyramids"
Bot: "Ø¹Ø§ÙŠØ² Ø£Ø²ÙˆØ± Ø§Ù„Ø£Ù‡Ø±Ø§Ù…Ø§Øª"
```

### Setting Up the Bot

1. **Create a Telegram Bot**:
   - Message @BotFather on Telegram
   - Use `/newbot` command
   - Get your bot token

2. **Configure the Bot**:
   ```python
   bot = telebot.TeleBot("YOUR_TELEGRAM_BOT_TOKEN")
   ```

3. **Run the Bot**:
   ```bash
   python telegram_bot.py
   ```

## ğŸ“Š Dataset Information

The training dataset contains **893 parallel sentences** covering:
- Daily conversations
- Cultural expressions
- Common phrases and idioms
- Egyptian-specific terminology

### Sample Data Points
| English | Egyptian Arabic |
|---------|-----------------|
| "I want to tell you something funny" | "Ø¹Ø§ÙŠØ² Ø§Ø­ÙƒÙŠÙ„ÙƒÙ… Ø­Ø§Ø¬Ù‡ ØºØ±ÙŠØ¨Ù‡ Ø¬Ø¯Ø§Ù‹" |
| "I have never visited the Pyramids" | "Ø¹Ù…Ø±Ù‰ Ù…Ø§ Ø²Ø±Øª Ø§Ù„Ø§Ù‡Ø±Ø§Ù…Ø§Øª" |
| "People from all over the world" | "Ø§Ù„Ù†Ø§Ø³ Ù…Ù† ÙƒÙ„ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…" |

## ğŸš€ Performance & Results

### Training Metrics
- **Training Loss**: Converged after 10 epochs
- **Memory Usage**: ~12GB VRAM with 4-bit quantization
- **Training Time**: ~2-3 hours on modern GPU

### Translation Quality
The model demonstrates excellent performance in:
- âœ… Maintaining cultural context
- âœ… Using appropriate Egyptian dialect forms
- âœ… Handling colloquial expressions
- âœ… Preserving meaning and tone

## ğŸ› ï¸ Advanced Usage

### Custom Training
To train on your own dataset:

1. **Prepare Data**: Format as CSV with "English Sentence" and "Arabic Sentence" columns
2. **Update Dataset Path**: Modify `data.csv` path in Part 2
3. **Adjust Parameters**: Tune LoRA and training parameters in Part 3
4. **Run Pipeline**: Execute notebooks sequentially

### Model Deployment
```python
# Save model for deployment
model.save_pretrained_merged("English2Egyptian", 
                           tokenizer, 
                           save_method="merged_16bit")

# Load for inference
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="English2Egyptian",
    max_seq_length=2048,
    dtype=None,
)
```

## ğŸ“‹ Requirements

### Core Dependencies
```
unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git
torch>=2.0.0
transformers>=4.30.0
datasets>=2.0.0
peft>=0.4.0
trl>=0.4.0
bitsandbytes>=0.39.0
accelerate>=0.20.0
pyTelegramBotAPI>=4.0.0
pandas>=1.3.0
```

### Hardware Requirements
- **Minimum**: 12GB VRAM GPU
- **Recommended**: 16GB+ VRAM GPU (RTX 3090, A100, etc.)
- **RAM**: 16GB+ system RAM
- **Storage**: 20GB+ free space

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **ğŸ› Bug Reports**: Open an issue with detailed description
2. **âœ¨ Feature Requests**: Suggest new features or improvements
3. **ğŸ“ Documentation**: Help improve documentation
4. **ğŸ”§ Code**: Submit pull requests with improvements

### Development Setup
```bash
git clone https://github.com/your-username/Gemma-2-9b-it-finetuning-Telegram-bot.git
cd Gemma-2-9b-it-finetuning-Telegram-bot
pip install -r requirements.txt
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Google**: For the amazing Gemma-2-9B model
- **Unsloth**: For the efficient fine-tuning framework
- **Hugging Face**: For the transformers library
- **Egyptian Arabic Community**: For linguistic insights and validation

## ğŸ“ Contact & Support

- **ğŸŒ Website**: [momenwalied.camitai.com](https://momenwalied.camitai.com/)
- **GitHub Issues**: [Report bugs or request features](https://github.com/your-username/Gemma-2-9b-it-finetuning-Telegram-bot/issues)
- **Discussions**: [Join the community discussion](https://github.com/your-username/Gemma-2-9b-it-finetuning-Telegram-bot/discussions)

---

<div align="center">

**Made with â¤ï¸ for the Egyptian Arabic NLP community**

*If this project helped you, please consider giving it a â­!*

</div>
